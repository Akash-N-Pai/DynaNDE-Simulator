{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5ef78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'flame-moe-290m_runid31066_epoch1080_layer2_shard0-0_512tokens.pt' and 'flame-moe-290m_runid31066_epoch1080_layer2_shard0-0_512tokens.txt'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Set print options to show all elements without truncation\n",
    "torch.set_printoptions(threshold=float('inf'))\n",
    "\n",
    "model = \"flame-moe-290m\"\n",
    "runid, epoch, layer = 31066, 1080, 2\n",
    "shard = \"0-0.pt\"\n",
    "\n",
    "samples = torch.load(Path(f\"samples/{model}/{runid}\", shard), map_location=\"cpu\")\n",
    "actives = torch.load(Path(f\"actives/{model}/{runid}/{epoch}/{layer}\", shard), map_location=\"cpu\")\n",
    "scores, indices = actives\n",
    "\n",
    "# Slice to first 512 tokens\n",
    "samples_sliced = samples[:512]\n",
    "scores_sliced = scores[:512]\n",
    "indices_sliced = indices[:512]\n",
    "\n",
    "# Create output filename with metadata\n",
    "shard_name = shard.replace('.pt', '')  # Remove .pt extension from shard\n",
    "output_filename = f\"{model}_runid{runid}_epoch{epoch}_layer{layer}_shard{shard_name}_512tokens\"\n",
    "\n",
    "# Save tensors to a .pt file with metadata\n",
    "output_data = {\n",
    "    'model': model,\n",
    "    'runid': runid,\n",
    "    'epoch': epoch,\n",
    "    'layer': layer,\n",
    "    'shard': shard,\n",
    "    'samples': samples_sliced,\n",
    "    'scores': scores_sliced,\n",
    "    'indices': indices_sliced\n",
    "}\n",
    "torch.save(output_data, f'{output_filename}.pt')\n",
    "\n",
    "# Save formatted output to a text file\n",
    "with open(f'{output_filename}.txt', 'w') as f:\n",
    "    # Write metadata header\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "    f.write(\"METADATA\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "    f.write(f\"Model: {model}\\n\")\n",
    "    f.write(f\"Run ID: {runid}\\n\")\n",
    "    f.write(f\"Epoch: {epoch}\\n\")\n",
    "    f.write(f\"Layer: {layer}\\n\")\n",
    "    f.write(f\"Shard: {shard}\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "    \n",
    "    # Write data\n",
    "    f.write(\"samples\".center(40, \"-\") + \"\\n\")\n",
    "    f.write(f\"{samples_sliced.shape}\\n\")\n",
    "    f.write(f\"{samples_sliced}\\n\\n\")\n",
    "    \n",
    "    f.write(\"scores\".center(40, \"-\") + \"\\n\")\n",
    "    f.write(f\"{scores_sliced.shape}\\n\")\n",
    "    f.write(f\"{scores_sliced}\\n\\n\")\n",
    "    \n",
    "    f.write(\"indices\".center(40, \"-\") + \"\\n\")\n",
    "    f.write(f\"{indices_sliced.shape}\\n\")\n",
    "    f.write(f\"{indices_sliced}\\n\")\n",
    "\n",
    "print(f\"Data saved to '{output_filename}.pt' and '{output_filename}.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d867a19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved to 'flame-moe-290m_runid31066_epoch1080_layer2_shard0-0_512tokens.csv'\n",
      "Format: layer_id=0, token_id (0-511), expert_0, expert_1, ..., expert_63 (6 decimal places)\n",
      "Total rows: 513 (including header)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import csv\n",
    "\n",
    "# Load the saved .pt file\n",
    "input_filename = \"flame-moe-290m_runid31066_epoch1080_layer2_shard0-0_512tokens\"\n",
    "data = torch.load(f'{input_filename}.pt')\n",
    "\n",
    "# Extract data\n",
    "samples = data['samples']\n",
    "scores = data['scores']\n",
    "indices = data['indices']\n",
    "\n",
    "# Create CSV filename\n",
    "csv_filename = f'{input_filename}.csv'\n",
    "num_experts = 64  # Total number of experts (0-63)\n",
    "\n",
    "with open(csv_filename, 'w', newline='') as csvfile:\n",
    "    # Create header\n",
    "    header = ['layer_id', 'token_id'] + [f'expert_{i}' for i in range(num_experts)]\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # Write data for each token\n",
    "    for token_idx in range(len(samples)):\n",
    "        # Use layer_id = 0 and token_idx (0 to 511)\n",
    "        row = [0, token_idx]\n",
    "        \n",
    "        # Initialize all experts with 0.000000 (formatted)\n",
    "        expert_scores = [0.0] * num_experts\n",
    "        \n",
    "        # Fill in the scores for active experts\n",
    "        for i in range(len(indices[token_idx])):\n",
    "            expert_id = int(indices[token_idx][i].item())\n",
    "            score = float(scores[token_idx][i].item())\n",
    "            expert_scores[expert_id] = score\n",
    "        \n",
    "        # Format all expert scores to 6 decimal places\n",
    "        formatted_scores = [f'{score:.6f}' for score in expert_scores]\n",
    "        row.extend(formatted_scores)\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"CSV saved to '{csv_filename}'\")\n",
    "print(f\"Format: layer_id=0, token_id (0-511), expert_0, expert_1, ..., expert_63 (6 decimal places)\")\n",
    "print(f\"Total rows: {len(samples) + 1} (including header)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35116425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics saved to:\n",
      "  - flame-moe-290m_runid31066_epoch1080_layer2_shard0-0_512tokens_top2_stats.txt (formatted text)\n",
      "  - flame-moe-290m_runid31066_epoch1080_layer2_shard0-0_512tokens_top2_stats.csv (CSV format)\n",
      "\n",
      "Total tokens analyzed: 512\n",
      "Total expert assignments (top 2): 1024\n",
      "\n",
      "Gating Mechanism Output:\n",
      "  Expert_Activ: 64\n",
      "  Expert_GPU:   4\n",
      "  Expert_MD:    60\n",
      "  H (Gating Output): 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Load the CSV file\n",
    "csv_filename = \"flame-moe-290m_runid31066_epoch1080_layer2_shard0-0_512tokens.csv\"\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Get expert columns (expert_0 to expert_63)\n",
    "expert_columns = [f'expert_{i}' for i in range(64)]\n",
    "\n",
    "# Initialize counter for each expert\n",
    "expert_counts = {i: 0 for i in range(64)}\n",
    "\n",
    "# For each token (row), find top 2 experts\n",
    "for idx, row in df.iterrows():\n",
    "    # Get scores for all experts\n",
    "    expert_scores = row[expert_columns].values\n",
    "    \n",
    "    # Get indices of top 2 experts\n",
    "    top_2_indices = np.argsort(expert_scores)[-2:][::-1]  # Descending order\n",
    "    \n",
    "    # Count each of the top 2 experts\n",
    "    for expert_id in top_2_indices:\n",
    "        expert_counts[expert_id] += 1\n",
    "\n",
    "# Sort by count (descending)\n",
    "sorted_experts = sorted(expert_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "total_assignments = sum(expert_counts.values())\n",
    "\n",
    "# Calculate Expert_Activ (number of experts with at least one token)\n",
    "expert_activ = sum(1 for count in expert_counts.values() if count > 0)\n",
    "\n",
    "# Gating Mechanism Parameters\n",
    "BW_PCIe = 32  # Gbps\n",
    "BW_MD = 512   # Gbps\n",
    "alpha = 1.0   # Scaling factor (default = 1)\n",
    "\n",
    "# Calculate Expert_GPU and Expert_MD\n",
    "expert_gpu = round((BW_PCIe / (BW_MD + BW_PCIe)) * expert_activ)\n",
    "expert_md = expert_activ - expert_gpu\n",
    "\n",
    "# Calculate Gating Mechanism Output (H)\n",
    "H = int(alpha * expert_gpu)\n",
    "\n",
    "# Create output filenames\n",
    "base_filename = csv_filename.replace('.csv', '')\n",
    "stats_txt_file = f\"{base_filename}_top2_stats.txt\"\n",
    "stats_csv_file = f\"{base_filename}_top2_stats.csv\"\n",
    "\n",
    "# Save to text file\n",
    "with open(stats_txt_file, 'w') as f:\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "    f.write(\"TOP 2 EXPERT TOKEN DISTRIBUTION\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "    f.write(f\"Total tokens: {len(df)}\\n\")\n",
    "    f.write(f\"Expected total (tokens × 2): {len(df) * 2}\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "    f.write(f\"{'Expert ID':<12} {'Token Count':<15} {'Percentage':<10}\\n\")\n",
    "    f.write(\"-\" * 60 + \"\\n\")\n",
    "    \n",
    "    for expert_id, count in sorted_experts:\n",
    "        percentage = (count / len(df)) * 100\n",
    "        f.write(f\"Expert {expert_id:<5} {count:<15} {percentage:>6.2f}%\\n\")\n",
    "    \n",
    "    f.write(\"-\" * 60 + \"\\n\")\n",
    "    f.write(f\"Total assignments: {total_assignments}\\n\")\n",
    "    \n",
    "    # Write Gating Mechanism Output\n",
    "    f.write(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "    f.write(\"GATING MECHANISM OUTPUT\\n\")\n",
    "    f.write(\"=\" * 60 + \"\\n\")\n",
    "    f.write(f\"Expert_Activ (experts with ≥1 token): {expert_activ}\\n\")\n",
    "    f.write(f\"\\nBandwidth Parameters:\\n\")\n",
    "    f.write(f\"  BW_PCIe: {BW_PCIe} Gbps\\n\")\n",
    "    f.write(f\"  BW_MD:   {BW_MD} Gbps\\n\")\n",
    "    f.write(f\"  Alpha (α): {alpha}\\n\")\n",
    "    f.write(f\"\\nCalculated Values:\\n\")\n",
    "    f.write(f\"  Expert_GPU: {expert_gpu}\\n\")\n",
    "    f.write(f\"  Expert_MD:  {expert_md}\\n\")\n",
    "    f.write(f\"  H (Gating Output): {H}\\n\")\n",
    "    f.write(f\"\\nFormulas Used:\\n\")\n",
    "    f.write(f\"  Expert_GPU = (BW_PCIe / (BW_MD + BW_PCIe)) × Expert_Activ\\n\")\n",
    "    f.write(f\"  Expert_GPU = ({BW_PCIe} / ({BW_MD} + {BW_PCIe})) × {expert_activ}\\n\")\n",
    "    f.write(f\"  Expert_MD = Expert_Activ - Expert_GPU\\n\")\n",
    "    f.write(f\"  H = α × Expert_GPU = {alpha} × {expert_gpu}\\n\")\n",
    "\n",
    "# Save to CSV file\n",
    "with open(stats_csv_file, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Expert_ID', 'Token_Count', 'Percentage', 'Rank'])\n",
    "    \n",
    "    for rank, (expert_id, count) in enumerate(sorted_experts, 1):\n",
    "        percentage = (count / len(df)) * 100\n",
    "        writer.writerow([expert_id, count, f\"{percentage:.2f}\", rank])\n",
    "\n",
    "print(f\"Statistics saved to:\")\n",
    "print(f\"  - {stats_txt_file} (formatted text)\")\n",
    "print(f\"  - {stats_csv_file} (CSV format)\")\n",
    "print(f\"\\nTotal tokens analyzed: {len(df)}\")\n",
    "print(f\"Total expert assignments (top 2): {total_assignments}\")\n",
    "print(f\"\\nGating Mechanism Output:\")\n",
    "print(f\"  Expert_Activ: {expert_activ}\")\n",
    "print(f\"  Expert_GPU:   {expert_gpu}\")\n",
    "print(f\"  Expert_MD:    {expert_md}\")\n",
    "print(f\"  H (Gating Output): {H}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
